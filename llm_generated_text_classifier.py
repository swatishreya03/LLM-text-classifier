# -*- coding: utf-8 -*-
"""LLM_Generated_Text_Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q9s1TmHTtTq9JtOIITfm5AOu2rqcadTt

# Importing Libraries
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers,models
import seaborn as sns
import nltk
from nltk.corpus import stopwords
import string
from nltk.stem.snowball import SnowballStemmer
from sklearn.feature_extraction.text import CountVectorizer
snowball = SnowballStemmer(language='english')
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier as XGB
from sklearn.metrics import confusion_matrix
from sklearn.svm import SVC
import transformers
from transformers import DistilBertTokenizer, TFDistilBertModel
from transformers import TFDistilBertForSequenceClassification, DistilBertConfig
from numba import cuda
from nltk.tokenize import word_tokenize
import tensorflow_text as tftext
import tensorflow_hub as tfhub
import keras_nlp
import keras
from keras import layers

"""# Loading the training CSV file"""

data = pd.read_csv('/content/train_drcat_01.csv')
data

data = data.drop(columns=['source','fold'])

data_classes = data.groupby('label').count()['text']
print(data_classes)
plt.title('class distribution in dataset')
plt.pie(data_classes , labels = ['0','1'] ,autopct='%0.0f%%')

"""The dataset has class imbalance. So, taking first 7000 rows of class 0 to make a dataset of 14000 rows with equal class quantity"""

data_0 = data[data['label']==0].iloc[0:7263,:]
data_1 = data[data['label']==1]

final_data = pd.concat([data_0,data_1])
final_data

plt.title('new class distribution of the dataset')
plt.pie(final_data.groupby('label').count()['text'],labels=['0','1'],autopct='%0.0f%%')

"""Final dataset is completed and ready to work on"""

final_data['text'].index = np.arange(0,final_data.shape[0])
final_data['text']

"""A few essays were too long (even 14000 words), so removed the long paragraphs as 75% of essays were below 229 words length. So, kept only the essays shorter than 500 words length.

# Splitting the dataset into train and test
"""

x_train, x_test ,y_train , y_test = train_test_split(final_data.iloc[:,0:2], final_data['label'],test_size=0.2)

len_train = []

for i in range(x_train.shape[0]):
    len_train.append(len(x_train.iloc[i,0]))
print('average characters per essay are ' , np.mean(len_train))

"""Average length of 10.2 is quite high !"""

x_train.groupby('label').count()

"""training data is balanced

# Preprocessing and fitting dataset in BERT model
"""

preprocessor1 = keras_nlp.models.DistilBertPreprocessor.from_preset(
    "distil_bert_base_en_uncased",
    sequence_length=512,
)

classifier = keras_nlp.models.DistilBertClassifier.from_preset(
    "distil_bert_base_en_uncased",
    num_classes=1,
)

classifier.backbone.trainable = False

classifier.compile(
    loss=['binary_crossentropy'],
    optimizer=keras.optimizers.Adam(1e-4),
    jit_compile=True,
    metrics = ['accuracy','AUC']
)


classifier.fit(x= x_train['text'].to_list(), y=x_train['label'], batch_size=8 ,epochs =2,
               validation_data = (x_test['text'].to_list(),x_test['label']))

final_test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')
final_test

"""# Prediction"""

pred = classifier.predict(final_test['text'].to_list())
pred

test_result = pred[:,0]
test_result

final_submission = pd.DataFrame(final_test['id'])
final_submission['generated'] = test_result
final_submission

final_submission.to_csv('submission.csv', index=False)

"""Results :
* XGBoost default : 0.276
* XGBoost learning rate 0.01 : 0.357
* XGBoost learning rate 0.005 : 0.442
* XGBoost learning rate 0.005 n_estimators 150 : 0.363
* SVM default : 0.325
* Logistic regression : 0.283
* DistilBERT learning rate 0.0001 : 0.369
* BERT learning rate 0.0001 6600 rows : 0.643
* BERT learning rate 0.0001 10660 rows (duplicates removed) : 0.668
* BERT learning rate 0.0005 10660 rows : 0.758
* BERT learning rate 0.0005 10660 rows without cleaning data (refer summary on first cell) : 0.758
* BERT learning rate 0.0007 11650 rows : 0.643
* BERT learning rate 0.0005 19000 rows : 0.499 (version 31)
* BERT learning rate 0.0005 DAIGT dataset : 0.484
* DistilBERT learning rate 0.0005 DAIGT dataset : 0.591
* DistilBERT learning rate 0.0005 DAIGT dataset without removing punctuation : 0.628
"""